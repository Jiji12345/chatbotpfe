{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder        \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from autocorrect import spell\n",
    "\n",
    "import os\n",
    "from six.moves import cPickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "sconfig = tf.ConfigProto()\n",
    "# sconfig.gpu_options.per_process_gpu_memory_fraction = 0.45\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 25\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 150\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "def process_str(string, bot_input=False, bot_output=False):\n",
    "    string = string.strip().lower()\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`:]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    string = string.split(\" \")\n",
    "    string = [re.sub(r\"[0-9]+\", \"NUM\", token) for token in string]\n",
    "    string = [stemmer.stem(re.sub(r'(.)\\1+', r'\\1\\1', token)) for token in string]\n",
    "    string = [spell(token).lower() for token in string]\n",
    "    # Truncate string\n",
    "    while True:\n",
    "        try:\n",
    "            string.remove(\"\")\n",
    "        except:\n",
    "            break\n",
    "    if(not bot_input and not bot_output):\n",
    "        string = string[0:MAX_LEN]\n",
    "    elif(bot_input):\n",
    "        string = string[0:MAX_LEN-1]\n",
    "        string.insert(0, \"</start>\")\n",
    "    else:\n",
    "        string = string[0:MAX_LEN-1]\n",
    "        string.insert(len(string), \"</end>\")\n",
    "    old_len = len(string)\n",
    "    for i in range((MAX_LEN) - len(string)):\n",
    "        string.append(\" </pad> \")\n",
    "    string = re.sub(\"\\s+\", \" \", \" \".join(string)).strip()\n",
    "    return string, old_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10407\n"
     ]
    }
   ],
   "source": [
    "data = cPickle.load(open(\"all_convos.pkl\", \"rb\"))\n",
    "print(len(data))\n",
    "user = [item[0] for item in data]\n",
    "bot = [item[1] for item in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(os.path.isfile(\"user_processed.pkl\")):\n",
    "    user = cPickle.load(open(\"user_processed.pkl\", \"rb\"))\n",
    "else:\n",
    "    user = [process_str(item) for item in user]\n",
    "    cPickle.dump(user, open(\"user_processed.pkl\", \"wb\"))\n",
    "\n",
    "if(os.path.isfile(\"bot_in_processed.pkl\")):\n",
    "    bot_inputs = cPickle.load(open(\"bot_in_processed.pkl\", \"rb\"))\n",
    "else:\n",
    "    bot_inputs = [process_str(item, bot_input=True) for item in bot]\n",
    "    cPickle.dump(bot_inputs, open(\"bot_in_processed.pkl\", \"wb\"))\n",
    "\n",
    "if(os.path.isfile(\"bot_out_processed.pkl\")):\n",
    "    bot_outputs = cPickle.load(open(\"bot_out_processed.pkl\", \"rb\"))\n",
    "else:\n",
    "    bot_outputs = [process_str(item, bot_output=True) for item in bot]\n",
    "    cPickle.dump(bot_outputs, open(\"bot_out_processed.pkl\", \"wb\"))\n",
    "    \n",
    "    \n",
    "user_lens = np.array([message[1] for message in user]).astype(np.int32)\n",
    "user = np.array([message[0] for message in user])\n",
    "\n",
    "bot_inp_lens = np.array([message[1] for message in bot_inputs]).astype(np.int32)\n",
    "bot_out_lens = np.array([message[1] for message in bot_outputs]).astype(np.int32)\n",
    "\n",
    "bot_inputs = np.array([message[0] for message in bot_inputs])\n",
    "bot_outputs = np.array([message[0] for message in bot_outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average user message: 10.602959546459115, average bot message: 13.784087633323724\n",
      "80th percentile of user lengths: 17.0, 80th percentile of bot lengths: 25.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Average user message: {}, average bot message: {}\".format(np.mean(user_lens), np.mean(bot_inp_lens)))\n",
    "print(\"80th percentile of user lengths: {}, 80th percentile of bot lengths: {}\".format(np.percentile(user_lens, 80), \n",
    "                                                                                       np.percentile(bot_inp_lens, 80)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = CountVectorizer()\n",
    "\n",
    "bow.fit(user.tolist() + bot_inputs.tolist())\n",
    "vocab = list(bow.vocabulary_.keys())\n",
    "vocab.insert(0, \"NUM\")\n",
    "vocab.insert(0, \"UNK\")\n",
    "vocab.insert(0, \"</end>\")\n",
    "vocab.insert(0, \"</start>\")\n",
    "vocab.insert(0, \"</pad>\")\n",
    "cPickle.dump(vocab, open(\"vocab\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ph = tf.placeholder(user.dtype, name=\"user_placeholder\")\n",
    "bot_inp_ph = tf.placeholder(bot_inputs.dtype, name=\"bot_inp_placeholder\")\n",
    "bot_out_ph = tf.placeholder(bot_outputs.dtype, name=\"bot_out_placeholder\")\n",
    "\n",
    "user_lens_ph = tf.placeholder(user_lens.dtype, shape=[None], name=\"user_len_placeholder\")\n",
    "bot_inp_lens_ph = tf.placeholder(bot_inp_lens.dtype, shape=[None], name=\"bot_inp_lens_placeholder\")\n",
    "bot_out_lens_ph = tf.placeholder(bot_out_lens.dtype, shape=[None], name=\"bot_out_lens_placeholder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_user = tf.data.Dataset.from_tensor_slices(user_ph)\n",
    "tf_bot_inp = tf.data.Dataset.from_tensor_slices(bot_inp_ph)\n",
    "tf_bot_out = tf.data.Dataset.from_tensor_slices(bot_out_ph)\n",
    "\n",
    "tf_user_lens = tf.data.Dataset.from_tensor_slices(user_lens_ph)\n",
    "tf_bot_inp_lens = tf.data.Dataset.from_tensor_slices(bot_inp_lens_ph)\n",
    "tf_bot_out_lens = tf.data.Dataset.from_tensor_slices(bot_out_lens_ph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\abiir\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\lookup_ops.py:1137: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\abiir\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:358: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "with tf.device(\"/cpu:0\"), tf.name_scope(\"data\"):\n",
    "    words = tf.contrib.lookup.index_table_from_tensor(mapping=tf.constant(vocab), default_value=3)\n",
    "    inverse = tf.contrib.lookup.index_to_string_table_from_tensor(mapping=tf.constant(vocab), default_value=\"UNK\", name=\"inverse_op\")\n",
    "\n",
    "    tf_user = tf_user.map(lambda string: tf.string_split([string])).map(lambda tokens: (words.lookup(tokens)))\n",
    "    tf_bot_inp = tf_bot_inp.map(lambda string: tf.string_split([string])).map(lambda tokens: (words.lookup(tokens)))\n",
    "    tf_bot_out = tf_bot_out.map(lambda string: tf.string_split([string])).map(lambda tokens: (words.lookup(tokens)))\n",
    "    \n",
    "    data = tf.data.Dataset.zip((tf_user, tf_bot_inp, tf_bot_out, tf_user_lens, tf_bot_inp_lens, tf_bot_out_lens))\n",
    "    data = data.shuffle(buffer_size=256).batch(BATCH_SIZE)\n",
    "    data = data.prefetch(10)\n",
    "    data_iterator = tf.data.Iterator.from_structure(data.output_types, data.output_shapes,\n",
    "                                                   None, data.output_classes)\n",
    "    train_init_op = data_iterator.make_initializer(data, name='dataset_init')\n",
    "    user_doc, bot_inp_doc, bot_out_doc, user_len, bot_inp_len, bot_out_len = data_iterator.get_next()\n",
    "    user_doc = tf.sparse_tensor_to_dense(user_doc)\n",
    "    bot_inp_doc = tf.sparse_tensor_to_dense(bot_inp_doc)\n",
    "    bot_out_doc = tf.sparse_tensor_to_dense(bot_out_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-81ce917076ab>:5: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"embedding\"):\n",
    "    embedding = tf.get_variable(\"embedding\", [len(vocab), 200], initializer=tf.glorot_uniform_initializer())\n",
    "    \n",
    "    embedded_user = tf.nn.embedding_lookup(embedding, user_doc)\n",
    "    embedded_user_dropout = tf.nn.dropout(embedded_user, 0.7)\n",
    "    \n",
    "    embedded_bot_inp = tf.nn.embedding_lookup(embedding, bot_inp_doc)\n",
    "    embedded_bot_inp_dropout = tf.nn.dropout(embedded_bot_inp, 0.7)\n",
    "    \n",
    "    embedded_user_dropout = tf.reshape(embedded_user_dropout, [-1, MAX_LEN, 200])\n",
    "    embedded_bot_inp_dropout = tf.reshape(embedded_bot_inp_dropout, [-1, MAX_LEN, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-aaef584844db>:3: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-12-aaef584844db>:11: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\abiir\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\abiir\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"encoder\"):\n",
    "    # Build RNN cell\n",
    "    encoder_GRU = tf.nn.rnn_cell.GRUCell(128)\n",
    "    encoder_cell_fw = tf.nn.rnn_cell.DropoutWrapper(encoder_GRU, input_keep_prob=0.7, \n",
    "                                                 output_keep_prob=0.7, state_keep_prob=0.9)\n",
    "    \n",
    "    encoder_cell_bw = tf.nn.rnn_cell.DropoutWrapper(encoder_GRU, input_keep_prob=0.7, \n",
    "                                                 output_keep_prob=0.7, state_keep_prob=0.9)\n",
    "    encoder_outputs, encoder_state = tf.nn.bidirectional_dynamic_rnn(\n",
    "        encoder_cell_fw, encoder_cell_bw, embedded_user_dropout,\n",
    "        sequence_length=user_len, dtype=tf.float32)\n",
    "    encoder_state = tf.concat(encoder_state, -1)\n",
    "    encoder_outputs = tf.concat(encoder_outputs, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mechanism = tf.contrib.seq2seq.LuongAttention(256, encoder_outputs,\n",
    "    memory_sequence_length=user_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"projection\"):\n",
    "    projection_layer = tf.layers.Dense(\n",
    "    len(vocab), use_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"decoder\"):\n",
    "    decoder_GRU = tf.nn.rnn_cell.GRUCell(256)\n",
    "    decoder_cell = tf.nn.rnn_cell.DropoutWrapper(decoder_GRU, input_keep_prob=0.7, \n",
    "                                                 output_keep_prob=0.7, state_keep_prob=0.9)\n",
    "    decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism,\n",
    "                                                       attention_layer_size=128)\n",
    "    \n",
    "    decoder_initial_state = decoder_cell.zero_state(BATCH_SIZE, tf.float32).clone(\n",
    "                                cell_state=encoder_state)\n",
    "    # Helper for use during training\n",
    "    # During training we feed the decoder\n",
    "    # the target sequence\n",
    "    # However, during testing we use the decoder's\n",
    "    # last output\n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "        embedded_bot_inp_dropout, bot_inp_len)\n",
    "    # Decoder\n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        decoder_cell, helper, decoder_initial_state,\n",
    "        output_layer=projection_layer)\n",
    "    # Dynamic decoding\n",
    "    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder)\n",
    "    logits = outputs.rnn_output\n",
    "    translations = outputs.sample_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.reshape(bot_out_doc,\n",
    "                                                                    [-1, MAX_LEN]), logits=logits)\n",
    "    mask = tf.sequence_mask(bot_out_len, dtype=tf.float32)\n",
    "    train_loss = (tf.reduce_sum(loss * mask) / BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam with gradient clipping and learning rate scheduling using cosine decay + restarts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('Adam'):\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    inc_gstep = tf.assign(global_step,global_step + 1)\n",
    "    learning_rate = tf.train.cosine_decay_restarts(0.001, global_step, 550, t_mul=1.1)\n",
    "    adam_optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    adam_gradients, v = zip(*adam_optimizer.compute_gradients(train_loss))\n",
    "    adam_gradients, _ = tf.clip_by_global_norm(adam_gradients, 10.0)\n",
    "    adam_optimize = adam_optimizer.apply_gradients(zip(adam_gradients, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"inference\"):\n",
    "    # Helper\n",
    "    # Start token is 1, which is the </start> token\n",
    "    # End token is 2\n",
    "    helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "        embedding,\n",
    "        tf.fill([BATCH_SIZE], 1), 2)\n",
    "\n",
    "    # Decoder\n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        decoder_cell, helper, decoder_initial_state,\n",
    "        output_layer=projection_layer)\n",
    "    # Dynamic decoding\n",
    "    test_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        decoder, maximum_iterations=10)\n",
    "    test_translations = tf.identity(test_outputs.sample_id, name=\"word_ids\")\n",
    "    test_words = tf.identity(inverse.lookup(tf.cast(test_translations, tf.int64)), name=\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testBot(sess):\n",
    "    text = [\"Hello\"] + [\"\"] * (BATCH_SIZE - 1)\n",
    "    num_text = len(text)\n",
    "    text = [process_str(sentence) for sentence in text]\n",
    "    text_len = np.array([item[1] for item in text]).astype(np.int32)\n",
    "    text = np.array([item[0] for item in text])\n",
    "    \n",
    "    user_test_ph = tf.placeholder(text.dtype)\n",
    "    user_test_lens_ph = tf.placeholder(text_len.dtype)\n",
    "    \n",
    "    tf_user_test = tf.data.Dataset.from_tensor_slices(user_test_ph).map(lambda string: tf.string_split([string])).map(lambda tokens: (words.lookup(tokens)))\n",
    "    tf_user_test_lens = tf.data.Dataset.from_tensor_slices(user_test_lens_ph)\n",
    "    \n",
    "    test_data = tf.data.Dataset.zip((tf_user_test, tf_bot_inp, tf_bot_out,\n",
    "                                     tf_user_test_lens, tf_bot_inp_lens, tf_bot_out_lens))\n",
    "    \n",
    "    test_data = test_data.batch(num_text).prefetch(1)\n",
    "    test_init_op = data_iterator.make_initializer(test_data)\n",
    "    \n",
    "    sess.run(test_init_op, feed_dict={\n",
    "        user_test_ph: user,\n",
    "        bot_inp_ph: bot_inputs[0:num_text],\n",
    "        bot_out_ph: bot_outputs[0:num_text],\n",
    "        user_test_lens_ph: user_lens,\n",
    "        bot_inp_lens_ph: bot_inp_lens[0:num_text],\n",
    "        bot_out_lens_ph: bot_out_lens[0:num_text]\n",
    "    })\n",
    "    translations_text = sess.run(inverse.lookup(tf.cast(test_translations, tf.int64)))\n",
    "    return translations_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('summaries'):\n",
    "    tf.summary.scalar('Loss', train_loss)\n",
    "    tf.summary.scalar('LR', learning_rate)\n",
    "    merged = tf.summary.merge_all()\n",
    "    config = projector.ProjectorConfig()\n",
    "    embedding_vis = config.embeddings.add()\n",
    "    embedding_vis.tensor_name = embedding.name\n",
    "    vocab_str = '\\n'.join(vocab)\n",
    "    metadata = pd.Series(vocab)\n",
    "    metadata.name = \"label\"\n",
    "    metadata.to_csv(\"checkpoints/metadata.tsv\", sep=\"\\t\", header=True, index_label=\"index\")\n",
    "    embedding_vis.metadata_path = 'metadata.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started training\n",
      "WARNING:tensorflow:From <ipython-input-21-37bd1b160732>:16: InitializableLookupTableBase.init (from tensorflow.python.ops.lookup_ops) is deprecated and will be removed after 2018-12-15.\n",
      "Instructions for updating:\n",
      "Use `initializer` instead.\n",
      "Epoch 0: Loss(Mean): 69.11994171142578 Loss(Std): 14.234908103942871\n",
      "[b'UNK' b'UNK' b'UNK' b'UNK' b'UNK' b'UNK' b'UNK' b'UNK' b'UNK' b'UNK']\n",
      "Epoch 1: Loss(Mean): 61.73150634765625 Loss(Std): 6.845091342926025\n",
      "[b'UNK' b'UNK' b'have' b'UNK' b'UNK' b'UNK' b'UNK' b'UNK' b'UNK' b'UNK']\n",
      "Epoch 2: Loss(Mean): 58.72270965576172 Loss(Std): 6.239071369171143\n",
      "[b'UNK' b'you' b'UNK' b'have' b'UNK' b'UNK' b'UNK' b'</end>' b'to' b'to']\n",
      "Epoch 3: Loss(Mean): 57.49359893798828 Loss(Std): 6.21344518661499\n",
      "[b'UNK' b'have' b'UNK' b'have' b'UNK' b'</end>' b'</end>' b'</end>'\n",
      " b'</end>' b'</end>']\n",
      "Epoch 4: Loss(Mean): 53.14601516723633 Loss(Std): 6.877599716186523\n",
      "[b'where' b'do' b'you' b'like' b'to' b'go' b'</end>' b'</end>' b'</end>'\n",
      " b'</end>']\n",
      "Epoch 5: Loss(Mean): 49.446136474609375 Loss(Std): 5.829213619232178\n",
      "[b'UNK' b'have' b'UNK' b'num' b'star' b'hotel' b'with' b'UNK' b'num'\n",
      " b'num']\n",
      "Epoch 6: Loss(Mean): 47.903717041015625 Loss(Std): 5.5314249992370605\n",
      "[b'UNK' b'have' b'UNK' b'num' b'day' b'package' b'UNK' b'</end>' b'UNK'\n",
      " b'</end>']\n",
      "Epoch 7: Loss(Mean): 46.998069763183594 Loss(Std): 5.440028190612793\n",
      "[b'where' b'are' b'you' b'go' b'to' b'mind' b'</end>' b'</end>' b'</end>'\n",
      " b'</end>']\n",
      "Epoch 8: Loss(Mean): 44.705116271972656 Loss(Std): 5.494061470031738\n",
      "[b'UNK' b'have' b'UNK' b'budget' b'</end>' b'UNK' b'</end>' b'trip' b'UNK'\n",
      " b'</end>']\n",
      "Epoch 9: Loss(Mean): 43.038368225097656 Loss(Std): 5.127781391143799\n",
      "[b'UNK' b'can' b'offer' b'you' b'UNK' b'num' b'day' b'package' b'in'\n",
      " b'mind']\n",
      "Epoch 10: Loss(Mean): 42.28602981567383 Loss(Std): 4.6920485496521\n",
      "[b'UNK' b'have' b'UNK' b'num' b'day' b'package' b'at' b'UNK' b'num'\n",
      " b'star']\n",
      "Epoch 11: Loss(Mean): 42.1968879699707 Loss(Std): 4.677088737487793\n",
      "[b'ism' b'afraid' b'we' b'have' b'UNK' b'num' b'day' b'package' b'at'\n",
      " b'UNK']\n",
      "Epoch 12: Loss(Mean): 41.33381271362305 Loss(Std): 5.119293212890625\n",
      "[b'UNK' b'have' b'UNK' b'num' b'star' b'hotel' b'in' b'porto' b'UNK'\n",
      " b'</end>']\n",
      "Epoch 13: Loss(Mean): 40.199825286865234 Loss(Std): 4.970727920532227\n",
      "[b'UNK' b'have' b'UNK' b'num' b'star' b'hotel' b'in' b'san' b'antonio'\n",
      " b'UNK']\n",
      "Epoch 14: Loss(Mean): 39.43758773803711 Loss(Std): 4.736103057861328\n",
      "[b'UNK' b'can' b'help' b'you' b'for' b'num' b'day' b'in' b'the' b'heart']\n",
      "Epoch 15: Loss(Mean): 39.143951416015625 Loss(Std): 4.404921531677246\n",
      "[b'where' b'are' b'you' b'be' b'leave' b'from' b'</end>' b'UNK' b'</end>'\n",
      " b'UNK']\n",
      "Epoch 16: Loss(Mean): 39.41892623901367 Loss(Std): 4.705824375152588\n",
      "[b'sure' b'UNK' b'we' b'do' b'not' b'have' b'ani' b'package' b'avail'\n",
      " b'from']\n",
      "Epoch 17: Loss(Mean): 38.61511993408203 Loss(Std): 4.61082649230957\n",
      "[b'sure' b'UNK' b'UNK' b'have' b'no' b'flight' b'to' b'help' b'you'\n",
      " b'with']\n",
      "Epoch 18: Loss(Mean): 37.863059997558594 Loss(Std): 4.55416202545166\n",
      "[b'where' b'would' b'you' b'like' b'to' b'go' b'</end>' b'</end>'\n",
      " b'</end>' b'</end>']\n",
      "Epoch 19: Loss(Mean): 37.286033630371094 Loss(Std): 4.197758674621582\n",
      "[b'UNK' b'can' b'book' b'you' b'UNK' b'num' b'day' b'package' b'from'\n",
      " b'septum']\n",
      "Epoch 20: Loss(Mean): 37.09287643432617 Loss(Std): 4.240796089172363\n",
      "[b'UNK' b'have' b'UNK' b'num' b'day' b'package' b'at' b'UNK' b'num'\n",
      " b'star']\n",
      "Epoch 21: Loss(Mean): 37.61078643798828 Loss(Std): 4.630959987640381\n",
      "[b'sure' b'UNK' b'where' b'would' b'you' b'like' b'to' b'go' b'</end>'\n",
      " b'</end>']\n",
      "Epoch 22: Loss(Mean): 36.958648681640625 Loss(Std): 4.677569389343262\n",
      "[b'UNK' b'have' b'UNK' b'num' b'day' b'package' b'at' b'UNK' b'num'\n",
      " b'star']\n",
      "Epoch 23: Loss(Mean): 36.347957611083984 Loss(Std): 4.066359043121338\n",
      "[b'UNK' b'have' b'UNK' b'num' b'day' b'package' b'at' b'num' b'star'\n",
      " b'hotel']\n",
      "Epoch 24: Loss(Mean): 35.810523986816406 Loss(Std): 4.028432846069336\n",
      "[b'UNK' b'have' b'UNK' b'num' b'day' b'package' b'at' b'UNK' b'num' b'num']\n",
      "Epoch 25: Loss(Mean): 35.497962951660156 Loss(Std): 4.323630332946777\n",
      "[b'UNK' b'can' b'book' b'you' b'num' b'day' b'in' b'san' b'antonio' b'for']\n",
      "Epoch 26: Loss(Mean): 35.94343185424805 Loss(Std): 3.848606586456299\n",
      "[b'UNK' b'have' b'sever' b'package' b'to' b'leave' b'from' b'septum'\n",
      " b'north' b'to']\n",
      "Epoch 27: Loss(Mean): 35.92765426635742 Loss(Std): 4.572323322296143\n",
      "[b'UNK' b'have' b'UNK' b'num' b'day' b'package' b'from' b'sept' b'num'\n",
      " b'num']\n",
      "Epoch 28: Loss(Mean): 35.394168853759766 Loss(Std): 4.3384270668029785\n",
      "[b'UNK' b'can' b'help' b'you' b'with' b'that' b'UNK' b'</end>' b'UNK'\n",
      " b'where']\n",
      "Epoch 29: Loss(Mean): 34.921749114990234 Loss(Std): 4.063081741333008\n",
      "[b'where' b'would' b'you' b'like' b'to' b'go' b'</end>' b'</end>'\n",
      " b'</end>' b'</end>']\n",
      "Epoch 30: Loss(Mean): 34.50009536743164 Loss(Std): 4.206514835357666\n",
      "[b'do' b'you' b'have' b'UNK' b'budget' b'</end>' b'</end>' b'</end>'\n",
      " b'</end>' b'</end>']\n",
      "Epoch 31: Loss(Mean): 34.2109489440918 Loss(Std): 4.084702968597412\n",
      "[b'UNK' b'can' b'help' b'you' b'UNK' b'num' b'day' b'stay' b'at' b'UNK']\n",
      "Epoch 32: Loss(Mean): 34.73676681518555 Loss(Std): 3.9170937538146973\n",
      "[b'UNK' b'have' b'UNK' b'num' b'day' b'package' b'in' b'the' b'heart'\n",
      " b'of']\n",
      "Epoch 33: Loss(Mean): 34.84233474731445 Loss(Std): 3.9272735118865967\n",
      "[b'do' b'you' b'have' b'UNK' b'budget' b'in' b'mind' b'</end>' b'</end>'\n",
      " b'UNK']\n",
      "Epoch 34: Loss(Mean): 34.34468460083008 Loss(Std): 4.311710357666016\n",
      "[b'do' b'you' b'have' b'UNK' b'budget' b'</end>' b'</end>' b'</end>'\n",
      " b'</end>' b'</end>']\n",
      "Epoch 35: Loss(Mean): 33.923133850097656 Loss(Std): 4.0144195556640625\n",
      "[b'where' b'would' b'you' b'like' b'to' b'go' b'</end>' b'</end>'\n",
      " b'</end>' b'</end>']\n",
      "Epoch 36: Loss(Mean): 33.5589599609375 Loss(Std): 4.002274513244629\n",
      "[b'ok' b'UNK' b'where' b'would' b'you' b'like' b'to' b'go' b'</end>'\n",
      " b'</end>']\n",
      "Epoch 37: Loss(Mean): 33.26161193847656 Loss(Std): 3.840768575668335\n",
      "[b'where' b'would' b'you' b'like' b'to' b'go' b'</end>' b'and' b'where'\n",
      " b'will']\n",
      "Epoch 38: Loss(Mean): 32.99943161010742 Loss(Std): 3.719864845275879\n",
      "[b'UNK' b'can' b'book' b'you' b'UNK' b'num' b'day' b'package' b'at' b'UNK']\n",
      "Epoch 39: Loss(Mean): 33.93082809448242 Loss(Std): 3.9458494186401367\n",
      "[b'UNK' b'have' b'UNK' b'num' b'num' b'star' b'hotel' b'avail' b'for'\n",
      " b'you']\n",
      "Epoch 40: Loss(Mean): 33.66543197631836 Loss(Std): 3.8830673694610596\n",
      "[b'UNK' b'have' b'UNK' b'num' b'day' b'package' b'at' b'UNK' b'num'\n",
      " b'star']\n",
      "Epoch 41: Loss(Mean): 33.29841613769531 Loss(Std): 4.168457984924316\n",
      "[b'UNK' b'have' b'UNK' b'num' b'day' b'package' b'from' b'septum' b'north'\n",
      " b'to']\n",
      "Epoch 42: Loss(Mean): 32.868316650390625 Loss(Std): 3.85445237159729\n",
      "[b'UNK' b'have' b'UNK' b'num' b'day' b'package' b'from' b'septum' b'north'\n",
      " b'to']\n",
      "Epoch 43: Loss(Mean): 32.564353942871094 Loss(Std): 4.025192737579346\n",
      "[b'UNK' b'can' b'help' b'you' b'with' b'that' b'UNK' b'have' b'UNK'\n",
      " b'great']\n",
      "Epoch 44: Loss(Mean): 32.21942138671875 Loss(Std): 3.5716381072998047\n",
      "[b'UNK' b'can' b'book' b'you' b'UNK' b'num' b'day' b'trip' b'to' b'kobe']\n",
      "Epoch 45: Loss(Mean): 32.016021728515625 Loss(Std): 3.6316378116607666\n",
      "[b'UNK' b'can' b'help' b'you' b'with' b'that' b'UNK' b'where' b'would'\n",
      " b'you']\n",
      "Epoch 46: Loss(Mean): 32.7047119140625 Loss(Std): 3.443768262863159\n",
      "[b'UNK' b'have' b'UNK' b'num' b'day' b'package' b'from' b'septum' b'north'\n",
      " b'to']\n",
      "Epoch 47: Loss(Mean): 32.86193084716797 Loss(Std): 4.164280414581299\n",
      "[b'ok' b'UNK' b'where' b'are' b'you' b'leave' b'from' b'</end>' b'</end>'\n",
      " b'and']\n",
      "Epoch 48: Loss(Mean): 32.58795166015625 Loss(Std): 3.9224860668182373\n",
      "[b'UNK' b'have' b'UNK' b'num' b'day' b'package' b'from' b'septum' b'num'\n",
      " b'to']\n",
      "Epoch 49: Loss(Mean): 32.13628005981445 Loss(Std): 3.910284996032715\n",
      "[b'UNK' b'have' b'UNK' b'num' b'day' b'package' b'to' b'num' b'day' b'UNK']\n",
      "Epoch 50: Loss(Mean): 31.846439361572266 Loss(Std): 3.742163896560669\n",
      "[b'do' b'you' b'have' b'UNK' b'budget' b'</end>' b'</end>' b'</end>'\n",
      " b'</end>' b'</end>']\n",
      "Epoch 51: Loss(Mean): 31.456388473510742 Loss(Std): 3.8127872943878174\n",
      "[b'hello' b'UNK' b'where' b'are' b'you' b'plan' b'from' b'</end>' b'to'\n",
      " b'guadalajara']\n",
      "Epoch 52: Loss(Mean): 31.25526237487793 Loss(Std): 3.80193829536438\n",
      "[b'where' b'would' b'you' b'like' b'to' b'go' b'</end>' b'and' b'where'\n",
      " b'would']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53: Loss(Mean): 31.095239639282227 Loss(Std): 3.635451555252075\n",
      "[b'UNK' b'have' b'UNK' b'num' b'day' b'package' b'at' b'UNK' b'num'\n",
      " b'star']\n",
      "Epoch 54: Loss(Mean): 31.841049194335938 Loss(Std): 3.5410170555114746\n",
      "[b'ok' b'UNK' b'UNK' b'have' b'UNK' b'num' b'day' b'package' b'avail'\n",
      " b'at']\n",
      "Epoch 55: Loss(Mean): 32.013153076171875 Loss(Std): 3.9668965339660645\n",
      "[b'UNK' b'have' b'num' b'day' b'UNK' b'num' b'star' b'hotel' b'UNK'\n",
      " b'</end>']\n",
      "Epoch 56: Loss(Mean): 31.770200729370117 Loss(Std): 3.9624085426330566\n",
      "[b'UNK' b'have' b'sever' b'option' b'avail' b'for' b'you' b'UNK' b'north'\n",
      " b'UNK']\n",
      "Epoch 57: Loss(Mean): 31.48149299621582 Loss(Std): 3.750105619430542\n",
      "[b'UNK' b'have' b'UNK' b'num' b'star' b'hotel' b'near' b'the' b'park'\n",
      " b'UNK']\n",
      "Epoch 58: Loss(Mean): 31.137592315673828 Loss(Std): 3.836656332015991\n",
      "[b'UNK' b'have' b'num' b'day' b'package' b'avail' b'for' b'num' b'day'\n",
      " b'at']\n",
      "Epoch 59: Loss(Mean): 30.774124145507812 Loss(Std): 3.732919454574585\n",
      "[b'when' b'would' b'you' b'like' b'to' b'go' b'</end>' b'</end>' b'</end>'\n",
      " b'</end>']\n",
      "Epoch 60: Loss(Mean): 30.545337677001953 Loss(Std): 3.714153289794922\n",
      "[b'UNK' b'can' b'help' b'you' b'with' b'that' b'UNK' b'where' b'are'\n",
      " b'you']\n",
      "Epoch 61: Loss(Mean): 30.322277069091797 Loss(Std): 3.6409082412719727\n",
      "[b'UNK' b'have' b'sever' b'package' b'avail' b'for' b'your' b'date'\n",
      " b'rang' b'UNK']\n",
      "Epoch 62: Loss(Mean): 30.209617614746094 Loss(Std): 3.262768507003784\n",
      "[b'ism' b'afraid' b'we' b'do' b'not' b'have' b'ani' b'flight' b'that'\n",
      " b'fit']\n",
      "Epoch 63: Loss(Mean): 31.14887237548828 Loss(Std): 3.5886940956115723\n",
      "[b'UNK' b'have' b'UNK' b'num' b'day' b'package' b'at' b'UNK' b'num'\n",
      " b'star']\n",
      "Epoch 64: Loss(Mean): 31.148197174072266 Loss(Std): 3.663151264190674\n",
      "[b'UNK' b'have' b'UNK' b'num' b'day' b'package' b'avail' b'at' b'the'\n",
      " b'num']\n",
      "Epoch 65: Loss(Mean): 30.92424964904785 Loss(Std): 3.6307287216186523\n",
      "[b'UNK' b'have' b'UNK' b'num' b'day' b'package' b'at' b'UNK' b'num'\n",
      " b'star']\n",
      "Epoch 66: Loss(Mean): 30.702491760253906 Loss(Std): 3.7115273475646973\n",
      "[b'UNK' b'have' b'UNK' b'num' b'day' b'package' b'at' b'UNK' b'num'\n",
      " b'star']\n",
      "Epoch 67: Loss(Mean): 30.333736419677734 Loss(Std): 3.7667834758758545\n",
      "[b'UNK' b'have' b'UNK' b'num' b'day' b'package' b'avail' b'for' b'you'\n",
      " b'UNK']\n",
      "Epoch 68: Loss(Mean): 30.120948791503906 Loss(Std): 3.898345470428467\n",
      "[b'UNK' b'have' b'UNK' b'num' b'star' b'hotel' b'with' b'UNK' b'num'\n",
      " b'num']\n",
      "Epoch 69: Loss(Mean): 29.887924194335938 Loss(Std): 3.8610095977783203\n",
      "[b'we' b'have' b'UNK' b'few' b'option' b'for' b'you' b'UNK' b'</end>'\n",
      " b'do']\n",
      "Epoch 70: Loss(Mean): 29.67611312866211 Loss(Std): 3.4348855018615723\n",
      "[b'UNK' b'have' b'UNK' b'num' b'day' b'package' b'avail' b'at' b'UNK'\n",
      " b'num']\n",
      "Epoch 71: Loss(Mean): 29.447826385498047 Loss(Std): 3.476278781890869\n",
      "[b'UNK' b'have' b'no' b'flight' b'avail' b'from' b'cairo' b'to' b'melburn'\n",
      " b'UNK']\n",
      "Epoch 72: Loss(Mean): 29.541534423828125 Loss(Std): 3.6879029273986816\n",
      "[b'okay' b'UNK' b'UNK' b'have' b'UNK' b'num' b'day' b'package' b'at'\n",
      " b'UNK']\n",
      "Epoch 73: Loss(Mean): 30.49631118774414 Loss(Std): 4.044980049133301\n",
      "[b'do' b'you' b'have' b'UNK' b'budget' b'</end>' b'</end>' b'</end>'\n",
      " b'</end>' b'</end>']\n",
      "Epoch 74: Loss(Mean): 30.408781051635742 Loss(Std): 3.6746912002563477\n",
      "[b'do' b'you' b'have' b'UNK' b'budget' b'</end>' b'budget' b'</end>'\n",
      " b'</end>' b'</end>']\n",
      "Epoch 75: Loss(Mean): 30.214250564575195 Loss(Std): 3.799679756164551\n",
      "[b'ism' b'afraid' b'we' b'do' b'not' b'have' b'ani' b'trip' b'avail'\n",
      " b'from']\n",
      "Epoch 76: Loss(Mean): 29.936939239501953 Loss(Std): 3.4469051361083984\n",
      "[b'UNK' b'have' b'UNK' b'few' b'option' b'in' b'curitiba' b'UNK' b'num'\n",
      " b'star']\n",
      "Epoch 77: Loss(Mean): 29.65401268005371 Loss(Std): 3.7942562103271484\n",
      "[b'UNK' b'do' b'not' b'have' b'aneth' b'that' b'work' b'UNK' b'</end>'\n",
      " b'</end>']\n",
      "Epoch 78: Loss(Mean): 29.398359298706055 Loss(Std): 3.6074326038360596\n",
      "[b'do' b'you' b'have' b'UNK' b'budget' b'</end>' b'budget' b'</end>'\n",
      " b'</end>' b'</end>']\n",
      "Epoch 79: Loss(Mean): 29.246665954589844 Loss(Std): 3.624143123626709\n",
      "[b'UNK' b'apolog' b'UNK' b'UNK' b'do' b'not' b'have' b'ani' b'flight'\n",
      " b'to']\n",
      "Epoch 80: Loss(Mean): 28.996774673461914 Loss(Std): 3.3698651790618896\n",
      "[b'UNK' b'have' b'not' b'avail' b'from' b'st' b'loud' b'to' b'punta'\n",
      " b'cana']\n",
      "Epoch 81: Loss(Mean): 28.859241485595703 Loss(Std): 3.344271421432495\n",
      "[b'when' b'would' b'you' b'like' b'to' b'leave' b'</end>' b'</end>'\n",
      " b'</end>' b'</end>']\n",
      "Epoch 82: Loss(Mean): 28.711341857910156 Loss(Std): 3.2412471771240234\n",
      "[b'UNK' b'can' b'offer' b'you' b'UNK' b'num' b'day' b'package' b'at'\n",
      " b'the']\n",
      "Epoch 83: Loss(Mean): 29.260374069213867 Loss(Std): 3.343242645263672\n",
      "[b'ok' b'UNK' b'UNK' b'have' b'UNK' b'love' b'package' b'at' b'UNK' b'num']\n",
      "Epoch 84: Loss(Mean): 29.809051513671875 Loss(Std): 3.4428863525390625\n",
      "[b'UNK' b'have' b'sever' b'package' b'avail' b'with' b'UNK' b'num' b'day'\n",
      " b'stay']\n",
      "Epoch 85: Loss(Mean): 29.73825454711914 Loss(Std): 3.6447176933288574\n",
      "[b'ok' b'UNK' b'UNK' b'have' b'num' b'day' b'in' b'santo' b'domingo' b'at']\n",
      "Epoch 86: Loss(Mean): 29.528488159179688 Loss(Std): 3.511624336242676\n",
      "[b'how' b'mani' b'people' b'will' b'be' b'traveling' b'</end>' b'</end>'\n",
      " b'</end>' b'</end>']\n",
      "Epoch 87: Loss(Mean): 29.254840850830078 Loss(Std): 3.614220380783081\n",
      "[b'UNK' b'have' b'sever' b'option' b'for' b'you' b'and' b'you' b'will'\n",
      " b'be']\n",
      "Epoch 88: Loss(Mean): 28.99540901184082 Loss(Std): 3.4559898376464844\n",
      "[b'UNK' b'have' b'UNK' b'love' b'trip' b'at' b'the' b'doubt' b'pasture'\n",
      " b'hotel']\n",
      "Epoch 89: Loss(Mean): 28.818084716796875 Loss(Std): 3.605625629425049\n",
      "[b'UNK' b'have' b'UNK' b'num' b'day' b'package' b'UNK' b'UNK' b'num'\n",
      " b'day']\n",
      "Epoch 90: Loss(Mean): 28.609350204467773 Loss(Std): 3.392172336578369\n",
      "[b'do' b'you' b'have' b'UNK' b'budget' b'in' b'mind' b'</end>' b'</end>'\n",
      " b'UNK']\n",
      "Epoch 91: Loss(Mean): 28.40765380859375 Loss(Std): 3.532628059387207\n",
      "[b'UNK' b'have' b'no' b'package' b'avail' b'from' b'beid' b'to' b'vitoria'\n",
      " b'UNK']\n",
      "Epoch 92: Loss(Mean): 28.208887100219727 Loss(Std): 3.423903226852417\n",
      "[b'ok' b'UNK' b'how' b'mani' b'people' b'will' b'you' b'be' b'travel'\n",
      " b'altogether']\n",
      "Epoch 93: Loss(Mean): 28.061420440673828 Loss(Std): 3.248666524887085\n",
      "[b'UNK' b'have' b'num' b'day' b'package' b'avail' b'in' b'punta' b'cana'\n",
      " b'UNK']\n",
      "Epoch 94: Loss(Mean): 28.004966735839844 Loss(Std): 3.263674259185791\n",
      "[b'unfortune' b'we' b'do' b'not' b'have' b'ani' b'flight' b'to' b'theed'\n",
      " b'from']\n",
      "Epoch 95: Loss(Mean): 28.945810317993164 Loss(Std): 3.2831125259399414\n",
      "[b'okay' b'UNK' b'where' b'would' b'you' b'like' b'to' b'go' b'</end>'\n",
      " b'</end>']\n",
      "Epoch 96: Loss(Mean): 29.072988510131836 Loss(Std): 3.3691165447235107\n",
      "[b'UNK' b'have' b'num' b'day' b'package' b'avail' b'for' b'you' b'UNK'\n",
      " b'it']\n",
      "Epoch 97: Loss(Mean): 29.074495315551758 Loss(Std): 3.6372182369232178\n",
      "[b'UNK' b'have' b'UNK' b'num' b'num' b'star' b'hotel' b'near' b'UNK'\n",
      " b'park']\n",
      "Epoch 98: Loss(Mean): 28.879056930541992 Loss(Std): 3.6025359630584717\n",
      "[b'do' b'you' b'have' b'UNK' b'budget' b'in' b'mind' b'</end>' b'</end>'\n",
      " b'do']\n",
      "Epoch 99: Loss(Mean): 28.681386947631836 Loss(Std): 3.456202983856201\n",
      "[b'UNK' b'have' b'UNK' b'num' b'day' b'package' b'at' b'UNK' b'num'\n",
      " b'star']\n",
      "Epoch 100: Loss(Mean): 28.466886520385742 Loss(Std): 3.4168519973754883\n",
      "[b'UNK' b'have' b'UNK' b'num' b'day' b'package' b'from' b'septum' b'north'\n",
      " b'to']\n",
      "Epoch 101: Loss(Mean): 28.193063735961914 Loss(Std): 3.5317139625549316\n",
      "[b'UNK' b'have' b'sever' b'option' b'for' b'you' b'UNK' b'it' b'is' b'UNK']\n",
      "Epoch 102: Loss(Mean): 27.99158477783203 Loss(Std): 3.4206438064575195\n",
      "[b'do' b'you' b'have' b'UNK' b'budget' b'</end>' b'</end>' b'</end>'\n",
      " b'budget' b'</end>']\n",
      "Epoch 103: Loss(Mean): 27.825298309326172 Loss(Std): 3.2319788932800293\n",
      "[b'ok' b'UNK' b'UNK' b'have' b'UNK' b'num' b'day' b'package' b'avail'\n",
      " b'at']\n",
      "Epoch 104: Loss(Mean): 27.658679962158203 Loss(Std): 3.5870182514190674\n",
      "[b'UNK' b'regret' b'to' b'inform' b'you' b'that' b'we' b'do' b'not'\n",
      " b'have']\n",
      "Epoch 105: Loss(Mean): 27.5283145904541 Loss(Std): 3.3867721557617188\n",
      "[b'ok' b'UNK' b'when' b'would' b'you' b'like' b'to' b'go' b'</end>'\n",
      " b'</end>']\n",
      "Epoch 106: Loss(Mean): 27.408462524414062 Loss(Std): 3.145630121231079\n",
      "[b'ok' b'UNK' b'where' b'are' b'you' b'leave' b'from' b'</end>' b'and'\n",
      " b'return']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107: Loss(Mean): 27.368621826171875 Loss(Std): 3.3353233337402344\n",
      "[b'UNK' b'have' b'UNK' b'num' b'day' b'package' b'at' b'UNK' b'num'\n",
      " b'star']\n",
      "Epoch 108: Loss(Mean): 28.433849334716797 Loss(Std): 3.2249884605407715\n",
      "[b'okay' b'UNK' b'where' b'would' b'you' b'like' b'to' b'go' b'</end>'\n",
      " b'</end>']\n",
      "Epoch 109: Loss(Mean): 28.5576114654541 Loss(Std): 3.5358362197875977\n",
      "[b'UNK' b'have' b'num' b'day' b'and' b'num' b'day' b'package' b'avail'\n",
      " b'at']\n",
      "Epoch 110: Loss(Mean): 28.393404006958008 Loss(Std): 3.3844258785247803\n",
      "[b'UNK' b'have' b'UNK' b'num' b'star' b'hotel' b'near' b'UNK' b'park'\n",
      " b'and']\n",
      "Epoch 111: Loss(Mean): 28.308584213256836 Loss(Std): 3.3580098152160645\n",
      "[b'do' b'you' b'have' b'UNK' b'budget' b'in' b'mind' b'</end>' b'</end>'\n",
      " b'</end>']\n",
      "Epoch 112: Loss(Mean): 28.08929443359375 Loss(Std): 3.418674945831299\n",
      "[b'UNK' b'have' b'UNK' b'few' b'option' b'for' b'you' b'that' b'is'\n",
      " b'your']\n",
      "Epoch 113: Loss(Mean): 27.918167114257812 Loss(Std): 3.4718117713928223\n",
      "[b'okay' b'UNK' b'have' b'UNK' b'fantast' b'num' b'star' b'hotel' b'near'\n",
      " b'UNK']\n",
      "Epoch 114: Loss(Mean): 27.714704513549805 Loss(Std): 3.510864496231079\n",
      "[b'UNK' b'have' b'UNK' b'trip' b'avail' b'from' b'septum' b'north' b'to'\n",
      " b'north']\n",
      "Epoch 115: Loss(Mean): 27.528413772583008 Loss(Std): 3.3671374320983887\n",
      "[b'ok' b'UNK' b'UNK' b'have' b'num' b'day' b'package' b'at' b'num' b'num']\n",
      "Epoch 116: Loss(Mean): 27.348506927490234 Loss(Std): 3.388080358505249\n",
      "[b'do' b'you' b'have' b'UNK' b'budget' b'</end>' b'</end>' b'do' b'you'\n",
      " b'have']\n",
      "Epoch 117: Loss(Mean): 27.118080139160156 Loss(Std): 3.3980917930603027\n",
      "[b'UNK' b'have' b'UNK' b'few' b'hotel' b'avail' b'in' b'curitiba' b'UNK'\n",
      " b'</end>']\n",
      "Epoch 118: Loss(Mean): 26.993440628051758 Loss(Std): 3.2643322944641113\n",
      "[b'ok' b'UNK' b'where' b'are' b'you' b'leave' b'from' b'</end>' b'</end>'\n",
      " b'</end>']\n",
      "Epoch 119: Loss(Mean): 26.89719009399414 Loss(Std): 3.1438112258911133\n",
      "[b'UNK' b'have' b'UNK' b'few' b'choice' b'in' b'goiania' b'UNK' b'but'\n",
      " b'we']\n",
      "Epoch 120: Loss(Mean): 26.788724899291992 Loss(Std): 3.227522850036621\n",
      "[b'UNK' b'have' b'UNK' b'num' b'day' b'package' b'avail' b'for' b'num'\n",
      " b'day']\n",
      "Epoch 121: Loss(Mean): 26.714632034301758 Loss(Std): 3.032696008682251\n",
      "[b'unfortune' b'UNK' b'do' b'not' b'have' b'ani' b'flight' b'that'\n",
      " b'dragon' b'alley']\n",
      "Epoch 122: Loss(Mean): 27.591650009155273 Loss(Std): 3.065891981124878\n",
      "[b'UNK' b'have' b'no' b'flight' b'out' b'of' b'st' b'loud' b'do' b'you']\n",
      "Epoch 123: Loss(Mean): 27.95789337158203 Loss(Std): 3.0699622631073\n",
      "[b'do' b'you' b'have' b'UNK' b'budget' b'for' b'thi' b'trip' b'</end>'\n",
      " b'</end>']\n",
      "Epoch 124: Loss(Mean): 27.896718978881836 Loss(Std): 3.425781011581421\n",
      "[b'ok' b'UNK' b'where' b'are' b'you' b'head' b'from' b'and' b'where'\n",
      " b'would']\n",
      "Epoch 125: Loss(Mean): 27.801788330078125 Loss(Std): 3.3522539138793945\n",
      "[b'ok' b'UNK' b'where' b'would' b'you' b'like' b'to' b'go' b'and' b'where']\n",
      "Epoch 126: Loss(Mean): 27.685420989990234 Loss(Std): 3.5592122077941895\n",
      "[b'UNK' b'have' b'UNK' b'num' b'day' b'package' b'that' b'work' b'for'\n",
      " b'you']\n",
      "Epoch 127: Loss(Mean): 27.544023513793945 Loss(Std): 3.2129218578338623\n",
      "[b'UNK' b'am' b'sorry' b'UNK' b'UNK' b'cannot' b'find' b'aneth' b'from'\n",
      " b'kabul']\n",
      "Epoch 128: Loss(Mean): 27.345699310302734 Loss(Std): 3.181602954864502\n",
      "[b'okay' b'UNK' b'do' b'you' b'have' b'UNK' b'budget' b'</end>' b'UNK'\n",
      " b'</end>']\n",
      "Epoch 129: Loss(Mean): 27.139991760253906 Loss(Std): 3.2672553062438965\n",
      "[b'do' b'you' b'have' b'UNK' b'budget' b'</end>' b'</end>' b'</end>'\n",
      " b'</end>' b'</end>']\n",
      "Epoch 130: Loss(Mean): 27.02849769592285 Loss(Std): 3.2088828086853027\n",
      "[b'do' b'you' b'have' b'UNK' b'budget' b'</end>' b'</end>' b'</end>'\n",
      " b'</end>' b'budget']\n",
      "Epoch 131: Loss(Mean): 26.804256439208984 Loss(Std): 3.2395520210266113\n",
      "[b'UNK' b'have' b'sever' b'option' b'for' b'you' b'do' b'you' b'want'\n",
      " b'to']\n",
      "Epoch 132: Loss(Mean): 26.62189483642578 Loss(Std): 3.415548086166382\n",
      "[b'do' b'you' b'have' b'UNK' b'budget' b'</end>' b'</end>' b'</end>'\n",
      " b'</end>' b'</end>']\n",
      "Epoch 133: Loss(Mean): 26.451913833618164 Loss(Std): 3.2358808517456055\n",
      "[b'UNK' b'have' b'UNK' b'few' b'possible' b'package' b'avail' b'UNK'\n",
      " b'but' b'you']\n",
      "Epoch 134: Loss(Mean): 26.316099166870117 Loss(Std): 3.1695730686187744\n",
      "[b'do' b'you' b'have' b'UNK' b'budget' b'</end>' b'</end>' b'</end>'\n",
      " b'</end>' b'</end>']\n",
      "Epoch 135: Loss(Mean): 26.265010833740234 Loss(Std): 3.1396796703338623\n",
      "[b'UNK' b'cannot' b'accommod' b'you' b'to' b'then' b'UNK' b'have' b'UNK'\n",
      " b'few']\n",
      "Epoch 136: Loss(Mean): 26.248586654663086 Loss(Std): 3.217885971069336\n",
      "[b'unfortune' b'there' b'are' b'no' b'result' b'for' b'thi' b'request'\n",
      " b'is' b'there']\n",
      "Epoch 137: Loss(Mean): 26.306228637695312 Loss(Std): 3.140071392059326\n",
      "[b'UNK' b'have' b'UNK' b'num' b'day' b'package' b'avail' b'for' b'you'\n",
      " b'UNK']\n",
      "Epoch 138: Loss(Mean): 27.38885498046875 Loss(Std): 3.2277650833129883\n",
      "[b'UNK' b'have' b'UNK' b'num' b'num' b'star' b'hotel' b'UNK' b'num' b'num']\n",
      "Epoch 139: Loss(Mean): 27.430747985839844 Loss(Std): 3.0857012271881104\n",
      "[b'ok' b'UNK' b'where' b'would' b'you' b'like' b'to' b'go' b'</end>'\n",
      " b'</end>']\n",
      "Epoch 140: Loss(Mean): 27.346508026123047 Loss(Std): 3.343327522277832\n",
      "[b'UNK' b'have' b'num' b'or' b'num' b'day' b'trip' b'avail' b'for' b'you']\n",
      "Epoch 141: Loss(Mean): 27.269296646118164 Loss(Std): 3.227869749069214\n",
      "[b'what' b'is' b'your' b'budget' b'</end>' b'</end>' b'</end>' b'UNK'\n",
      " b'</end>' b'</end>']\n",
      "Epoch 142: Loss(Mean): 27.168087005615234 Loss(Std): 3.2035210132598877\n",
      "[b'ok' b'UNK' b'UNK' b'have' b'UNK' b'num' b'day' b'package' b'at' b'UNK']\n",
      "Epoch 143: Loss(Mean): 26.959882736206055 Loss(Std): 3.4461519718170166\n",
      "[b'UNK' b'have' b'not' b'avail' b'from' b'torino' b'to' b'santo'\n",
      " b'domingo' b'UNK']\n",
      "Epoch 144: Loss(Mean): 26.938703536987305 Loss(Std): 3.464310646057129\n",
      "[b'UNK' b'have' b'num' b'day' b'at' b'the' b'num' b'star' b'hotel'\n",
      " b'richard']\n",
      "Epoch 145: Loss(Mean): 26.680530548095703 Loss(Std): 3.3459513187408447\n",
      "[b'what' b'is' b'your' b'budget' b'</end>' b'</end>' b'</end>' b'</end>'\n",
      " b'</end>' b'</end>']\n",
      "Epoch 146: Loss(Mean): 26.53438377380371 Loss(Std): 3.1276237964630127\n",
      "[b'UNK' b'have' b'num' b'day' b'at' b'UNK' b'num' b'star' b'hotel' b'in']\n",
      "Epoch 147: Loss(Mean): 26.406593322753906 Loss(Std): 3.147259473800659\n",
      "[b'okay' b'UNK' b'where' b'would' b'you' b'like' b'to' b'go' b'</end>'\n",
      " b'</end>']\n",
      "Epoch 148: Loss(Mean): 26.28437614440918 Loss(Std): 3.0573811531066895\n",
      "[b'okay' b'UNK' b'UNK' b'have' b'num' b'day' b'at' b'the' b'sunset'\n",
      " b'baron']\n",
      "Epoch 149: Loss(Mean): 26.109172821044922 Loss(Std): 3.1669440269470215\n",
      "[b'UNK' b'have' b'UNK' b'few' b'option' b'for' b'you' b'UNK' b'num' b'day']\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "print(\"Started training\")\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "save_dir = 'checkpoints/'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "save_path = os.path.join(save_dir, 'best_validation')\n",
    "\n",
    "sess = tf.InteractiveSession(config=sconfig)\n",
    "\n",
    "writer = tf.summary.FileWriter('./checkpoints', sess.graph)\n",
    "projector.visualize_embeddings(writer, config)\n",
    "\n",
    "\n",
    "sess.run([words.init, tf.global_variables_initializer(), inverse.init])\n",
    "step = 0\n",
    "\n",
    "for i in range(NUM_EPOCHS):\n",
    "    if(i % 10 == 0):\n",
    "        saver.save(sess=sess, save_path=save_path, write_meta_graph=True)\n",
    "    sess.run(train_init_op, feed_dict={\n",
    "        user_ph: user,\n",
    "        bot_inp_ph: bot_inputs,\n",
    "        bot_out_ph: bot_outputs,\n",
    "        user_lens_ph: user_lens,\n",
    "        bot_inp_lens_ph: bot_inp_lens,\n",
    "        bot_out_lens_ph: bot_out_lens\n",
    "    })\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            _, batch_loss, summary = sess.run([adam_optimize, train_loss, merged])\n",
    "            writer.add_summary(summary, i)\n",
    "            losses.append(batch_loss)\n",
    "        except tf.errors.InvalidArgumentError:\n",
    "            continue\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"Epoch {}: Loss(Mean): {} Loss(Std): {}\".format(i, np.mean(losses), np.std(losses)))\n",
    "            losses = []\n",
    "            break\n",
    "        sess.run(inc_gstep)\n",
    "        step += 1\n",
    "    print(testBot(sess)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
